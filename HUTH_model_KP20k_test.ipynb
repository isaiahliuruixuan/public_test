{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "407c4f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rn\n",
    "\n",
    "# The below is necessary in Python 3.2.3 onwards to\n",
    "# have reproducible behavior for certain hash-based operations.\n",
    "# See these references for further details:\n",
    "# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED\n",
    "# https://github.com/fchollet/keras/issues/2280#issuecomment-306959926\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '01'\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "\n",
    "np.random.seed(421)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "rn.seed(123451)\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "from data.datasets import *\n",
    "from eval import keras_metrics, metrics\n",
    "from nlp import tokenizer as tk\n",
    "from utils import info, preprocessing, postprocessing, plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03e7f034",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 23:07:57,844\tINFO\tKeras version 2.7.0\n",
      "2021-12-12 23:07:57,845\tINFO\tNumpy version 1.20.1\n",
      "2021-12-12 23:07:57,846\tINFO\tTensorflow version 2.7.0\n",
      "2021-12-12 23:07:57,847\tINFO\tLoading dataset...\n",
      "2021-12-12 23:07:57,848\tDEBUG\tInitialized dataset Semeval 2017 from folder data/Semeval2017\n",
      "2021-12-12 23:07:58,172\tDEBUG\tLoaded training set for dataset Semeval 2017\n",
      "2021-12-12 23:07:58,230\tDEBUG\tLoaded test set for dataset Semeval 2017\n",
      "2021-12-12 23:07:58,278\tDEBUG\tLoaded validation set for dataset Semeval 2017\n",
      "2021-12-12 23:07:59,401\tINFO\tDataset loaded. Preprocessing data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# LOGGING CONFIGURATION\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s\\t%(levelname)s\\t%(message)s',\n",
    "    level=logging.DEBUG)\n",
    "\n",
    "info.log_versions()\n",
    "\n",
    "# END LOGGING CONFIGURATION\n",
    "\n",
    "# GLOBAL VARIABLES\n",
    "\n",
    "SAVE_MODEL = False\n",
    "MODEL_PATH = \"models/mergernn.h5\"\n",
    "SHOW_PLOTS = False\n",
    "\n",
    "# END GLOBAL VARIABLES\n",
    "\n",
    "# Dataset and hyperparameters for each dataset\n",
    "\n",
    "DATASET = Semeval2017\n",
    "\n",
    "if DATASET == Semeval2017:\n",
    "    tokenizer = tk.tokenizers.nltk\n",
    "    DATASET_FOLDER = \"data/Semeval2017\"\n",
    "    MAX_DOCUMENT_LENGTH = 400\n",
    "    MAX_VOCABULARY_SIZE = 20000\n",
    "    EMBEDDINGS_SIZE = 300\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 10\n",
    "elif DATASET == Hulth:\n",
    "    tokenizer = tk.tokenizers.nltk\n",
    "    DATASET_FOLDER = \"data/Hulth2003\"\n",
    "    MAX_DOCUMENT_LENGTH = 550\n",
    "    MAX_VOCABULARY_SIZE = 20000\n",
    "    EMBEDDINGS_SIZE = 300\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 41\n",
    "else:\n",
    "    raise NotImplementedError(\"Can't set the hyperparameters: unknown dataset\")\n",
    "\n",
    "\n",
    "# END PARAMETERS\n",
    "\n",
    "logging.info(\"Loading dataset...\")\n",
    "\n",
    "data = DATASET(DATASET_FOLDER)\n",
    "\n",
    "train_doc_str, train_answer_str = data.load_train()\n",
    "test_doc_str, test_answer_str = data.load_test()\n",
    "val_doc_str, val_answer_str = data.load_validation()\n",
    "\n",
    "train_doc, train_answer = tk.tokenize_set(train_doc_str,train_answer_str,tokenizer)\n",
    "test_doc, test_answer = tk.tokenize_set(test_doc_str,test_answer_str,tokenizer)\n",
    "val_doc, val_answer = tk.tokenize_set(val_doc_str,val_answer_str,tokenizer)\n",
    "\n",
    "# Sanity check\n",
    "# logging.info(\"Sanity check: %s\",metrics.precision(test_answer,test_answer))\n",
    "\n",
    "logging.info(\"Dataset loaded. Preprocessing data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b920bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Opening JSON file\n",
    "\n",
    "# table = []\n",
    "kp20k_val_doc_str = {}\n",
    "kp20k_val_answer_str = {}\n",
    "count = 0\n",
    "with open('./data/KP20k/kp20k_validation.json', 'r') as f:\n",
    "    for line in f:\n",
    "        if count % 20 != 1:\n",
    "            count += 1\n",
    "            continue\n",
    "        item = json.loads(line)\n",
    "        kp20k_val_doc_str[count] = item['abstract']\n",
    "        kp20k_val_answer_str[count] = item['keyword'].split(';')\n",
    "#         table.append(item)\n",
    "        count += 1\n",
    "# print(kp20k_val_doc_str[67])        \n",
    "# print(kp20k_val_answer_str[67], '\\n')\n",
    "\n",
    "kp20k_train_doc_str = {}\n",
    "kp20k_train_answer_str = {}\n",
    "count = 0\n",
    "with open('./data/KP20k/kp20k_training.json', 'r') as f:\n",
    "    for line in f:\n",
    "        if count % 100 != 3:\n",
    "            count += 1\n",
    "            continue\n",
    "        item = json.loads(line)\n",
    "        kp20k_train_doc_str[count] = item['abstract']\n",
    "        kp20k_train_answer_str[count] = item['keyword'].split(';')\n",
    "#         table.append(item)\n",
    "        count += 1\n",
    "# print(kp20k_train_doc_str[4760])\n",
    "# print(kp20k_train_answer_str[4760], '\\n')\n",
    "    \n",
    "    \n",
    "kp20k_test_doc_str = {}\n",
    "kp20k_test_answer_str = {}\n",
    "count = 0\n",
    "with open('./data/KP20k/kp20k_testing.json', 'r') as f:\n",
    "    for line in f:\n",
    "        if count % 20 != 2:\n",
    "            count += 1\n",
    "            continue\n",
    "        item = json.loads(line)\n",
    "        kp20k_test_doc_str[count] = item['abstract']\n",
    "        kp20k_test_answer_str[count] = item['keyword'].split(';')\n",
    "#         table.append(item)\n",
    "        count += 1\n",
    "\n",
    "# print(kp20k_test_doc_str[60])\n",
    "# print(kp20k_test_answer_str[60], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7181d18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kp20k_train_doc, kp20k_train_answer = tk.tokenize_set(kp20k_train_doc_str, kp20k_train_answer_str, tokenizer)\n",
    "kp20k_val_doc, kp20k_val_answer = tk.tokenize_set(kp20k_val_doc_str, kp20k_val_answer_str, tokenizer)\n",
    "kp20k_test_doc, kp20k_test_answer = tk.tokenize_set(kp20k_test_doc_str, kp20k_test_answer_str, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d0370fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 23:08:56,077\tDEBUG\tFitting dictionary on 7309 documents...\n",
      "2021-12-12 23:08:56,396\tDEBUG\tDictionary fitting completed. Found 43642 unique tokens\n",
      "2021-12-12 23:08:56,681\tDEBUG\tLongest training document : 832 tokens\n",
      "2021-12-12 23:08:56,682\tDEBUG\tLongest test document :     443 tokens\n",
      "2021-12-12 23:08:56,682\tDEBUG\tLongest validation document : 693 tokens\n",
      "2021-12-12 23:08:57,531\tDEBUG\tTraining set samples size   : (5309, 800)\n",
      "2021-12-12 23:08:57,532\tDEBUG\tTraining set answers size   : (5309, 800, 3)\n",
      "2021-12-12 23:08:57,532\tDEBUG\tTest set samples size       : (1000, 800)\n",
      "2021-12-12 23:08:57,532\tDEBUG\tTest set answers size       : (1000, 800, 3) \n",
      "2021-12-12 23:08:57,533\tDEBUG\tValidation set samples size : (1000, 800)\n",
      "2021-12-12 23:08:57,533\tDEBUG\tValidation set answers size : (1000, 800, 3) \n",
      "2021-12-12 23:08:57,534\tDEBUG\tLoading GloVe pre-trained embeddings from glove.6B.300d.txt\n",
      "2021-12-12 23:09:15,525\tDEBUG\tTotal embeddings found: 400000.\n",
      "2021-12-12 23:09:15,526\tDEBUG\tBuilding embedding matrix of size [43643,300]...\n",
      "2021-12-12 23:09:15,874\tINFO\tData preprocessing complete.\n",
      "2021-12-12 23:09:16,117\tINFO\tMaximum possible recall: 0.4692694240432934\n"
     ]
    }
   ],
   "source": [
    "MAX_DOCUMENT_LENGTH = 800\n",
    "MAX_VOCABULARY_SIZE = 45000\n",
    "train_x,train_y,test_x,test_y,val_x,val_y,embedding_matrix = preprocessing.\\\n",
    "    prepare_sequential(kp20k_train_doc, kp20k_train_answer, \n",
    "                       kp20k_test_doc, kp20k_test_answer,\n",
    "                       kp20k_val_doc,kp20k_val_answer,\n",
    "                       max_document_length=MAX_DOCUMENT_LENGTH,\n",
    "                       max_vocabulary_size=MAX_VOCABULARY_SIZE,\n",
    "                       embeddings_size=EMBEDDINGS_SIZE)\n",
    "\n",
    "# weigh training examples: everything that's not class 0 (not kp)\n",
    "# gets a heavier score\n",
    "train_y_weights = np.argmax(train_y,axis=2) # this removes the one-hot representation\n",
    "train_y_weights[train_y_weights > 0] = 15\n",
    "train_y_weights[train_y_weights < 1] = 1\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "# train_y_weights = np.argmax(train_y, axis=2)\n",
    "# train_y_weights = np.reshape(class_weight.compute_sample_weight('balanced', train_y_weights.flatten()),\n",
    "#                              np.shape(train_y_weights))\n",
    "\n",
    "\n",
    "logging.info(\"Data preprocessing complete.\")\n",
    "logging.info(\"Maximum possible recall: %s\",\n",
    "             metrics.recall(kp20k_test_answer,\n",
    "                               postprocessing.get_words(kp20k_test_doc,postprocessing.undo_sequential(test_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2be4d59c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 23:09:16,136\tINFO\tLoading existing model from ./model/new_kp20k_mergebigru_1_8_epoch_10...\n",
      "2021-12-12 23:09:26,560\tINFO\tCompleted loading model from file\n",
      "2021-12-12 23:09:26,560\tINFO\tPredicting on test set...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 23s 683ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 23:09:49,757\tDEBUG\tShape of output array: (1000, 800, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###    Obtained Scores    ###\n",
      "###     (full dataset)    ###\n",
      "###\n",
      "### Precision : 0.0405\n",
      "### Recall    : 0.0146\n",
      "### F1        : 0.0215\n",
      "###                       ###\n",
      "###    Obtained Scores    ###\n",
      "###    (fixed dataset)    ###\n",
      "###\n",
      "### Precision : 0.1228\n",
      "### Recall    : 0.2311\n",
      "### F1        : 0.1603\n",
      "###                       ###\n",
      "###    Obtained Scores    ###\n",
      "### (full dataset,        ###\n",
      "###  pos patterns filter) ###\n",
      "###\n",
      "### Precision : 0.1616\n",
      "### Recall    : 0.0361\n",
      "### F1        : 0.0590\n",
      "###                       ###\n",
      "###    Obtained Scores    ###\n",
      "### (full dataset, top 5) ###\n",
      "###\n",
      "### Precision : 0.0535\n",
      "### Recall    : 0.0117\n",
      "### F1        : 0.0192\n",
      "###                       ###\n",
      "###    Obtained Scores    ###\n",
      "### (full dataset, top 10)###\n",
      "###\n",
      "### Precision : 0.0399\n",
      "### Recall    : 0.0132\n",
      "### F1        : 0.0198\n",
      "###                       ###\n",
      "###    Obtained Scores    ###\n",
      "### (full dataset, top 15)###\n",
      "###\n",
      "### Precision : 0.0399\n",
      "### Recall    : 0.0141\n",
      "### F1        : 0.0209\n",
      "###                       ###\n",
      "###                       ###\n",
      "###                       ###\n",
      "###       STEMMING        ###\n",
      "###                       ###\n",
      "###                       ###\n",
      "###    Obtained Scores    ###\n",
      "###     (full dataset)    ###\n",
      "###\n",
      "### Precision : 0.0432\n",
      "### Recall    : 0.0156\n",
      "### F1        : 0.0229\n",
      "###                       ###\n",
      "###    Obtained Scores    ###\n",
      "###    (fixed dataset)    ###\n",
      "###\n",
      "### Precision : 0.1228\n",
      "### Recall    : 0.2311\n",
      "### F1        : 0.1603\n",
      "###                       ###\n",
      "###    Obtained Scores    ###\n",
      "### (full dataset,        ###\n",
      "###  pos patterns filter) ###\n",
      "###\n",
      "### Precision : 0.1681\n",
      "### Recall    : 0.0375\n",
      "### F1        : 0.0614\n",
      "###                       ###\n",
      "###    Obtained Scores    ###\n",
      "### (full dataset, top 5) ###\n",
      "###\n",
      "### Precision : 0.0535\n",
      "### Recall    : 0.0117\n",
      "### F1        : 0.0192\n",
      "###                       ###\n",
      "###    Obtained Scores    ###\n",
      "### (full dataset, top 10)###\n",
      "###\n",
      "### Precision : 0.0429\n",
      "### Recall    : 0.0141\n",
      "### F1        : 0.0213\n",
      "###                       ###\n",
      "###    Obtained Scores    ###\n",
      "### (full dataset, top 15)###\n",
      "###\n",
      "### Precision : 0.0427\n",
      "### Recall    : 0.0151\n",
      "### F1        : 0.0223\n",
      "###                       ###\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = './model/new_kp20k_mergebigru_1_8_epoch_10'\n",
    "\n",
    "logging.info(\"Loading existing model from %s...\",MODEL_PATH)\n",
    "model = load_model(MODEL_PATH)\n",
    "logging.info(\"Completed loading model from file\")\n",
    "\n",
    "\n",
    "logging.info(\"Predicting on test set...\")\n",
    "output = model.predict(x=[test_x,test_x], verbose=1)\n",
    "logging.debug(\"Shape of output array: %s\",np.shape(output))\n",
    "\n",
    "obtained_tokens = postprocessing.undo_sequential(output)\n",
    "obtained_words = postprocessing.get_words(test_doc,obtained_tokens)\n",
    "\n",
    "precision = metrics.precision(test_answer,obtained_words)\n",
    "recall = metrics.recall(test_answer,obtained_words)\n",
    "f1 = metrics.f1(precision,recall)\n",
    "\n",
    "print(\"###    Obtained Scores    ###\")\n",
    "print(\"###     (full dataset)    ###\")\n",
    "print(\"###\")\n",
    "print(\"### Precision : %.4f\" % precision)\n",
    "print(\"### Recall    : %.4f\" % recall)\n",
    "print(\"### F1        : %.4f\" % f1)\n",
    "print(\"###                       ###\")\n",
    "\n",
    "keras_precision = keras_metrics.keras_precision(test_y,output)\n",
    "keras_recall = keras_metrics.keras_recall(test_y,output)\n",
    "keras_f1 = keras_metrics.keras_f1(test_y,output)\n",
    "\n",
    "print(\"###    Obtained Scores    ###\")\n",
    "print(\"###    (fixed dataset)    ###\")\n",
    "print(\"###\")\n",
    "print(\"### Precision : %.4f\" % keras_precision)\n",
    "print(\"### Recall    : %.4f\" % keras_recall)\n",
    "print(\"### F1        : %.4f\" % keras_f1)\n",
    "print(\"###                       ###\")\n",
    "\n",
    "clean_words = postprocessing.get_valid_patterns(obtained_words)\n",
    "\n",
    "precision = metrics.precision(test_answer,clean_words)\n",
    "recall = metrics.recall(test_answer,clean_words)\n",
    "f1 = metrics.f1(precision,recall)\n",
    "\n",
    "print(\"###    Obtained Scores    ###\")\n",
    "print(\"### (full dataset,        ###\")\n",
    "print(\"###  pos patterns filter) ###\")\n",
    "print(\"###\")\n",
    "print(\"### Precision : %.4f\" % precision)\n",
    "print(\"### Recall    : %.4f\" % recall)\n",
    "print(\"### F1        : %.4f\" % f1)\n",
    "print(\"###                       ###\")\n",
    "\n",
    "\n",
    "obtained_words_top = postprocessing.get_top_words(test_doc, output, 5)\n",
    "\n",
    "precision_top = metrics.precision(test_answer, obtained_words_top)\n",
    "recall_top = metrics.recall(test_answer, obtained_words_top)\n",
    "f1_top = metrics.f1(precision_top, recall_top)\n",
    "\n",
    "print(\"###    Obtained Scores    ###\")\n",
    "print(\"### (full dataset, top 5) ###\")\n",
    "print(\"###\")\n",
    "print(\"### Precision : %.4f\" % precision_top)\n",
    "print(\"### Recall    : %.4f\" % recall_top)\n",
    "print(\"### F1        : %.4f\" % f1_top)\n",
    "print(\"###                       ###\")\n",
    "\n",
    "obtained_words_top = postprocessing.get_top_words(test_doc, output, 10)\n",
    "\n",
    "precision_top = metrics.precision(test_answer, obtained_words_top)\n",
    "recall_top = metrics.recall(test_answer, obtained_words_top)\n",
    "f1_top = metrics.f1(precision_top, recall_top)\n",
    "\n",
    "print(\"###    Obtained Scores    ###\")\n",
    "print(\"### (full dataset, top 10)###\")\n",
    "print(\"###\")\n",
    "print(\"### Precision : %.4f\" % precision_top)\n",
    "print(\"### Recall    : %.4f\" % recall_top)\n",
    "print(\"### F1        : %.4f\" % f1_top)\n",
    "print(\"###                       ###\")\n",
    "\n",
    "obtained_words_top = postprocessing.get_top_words(test_doc, output, 15)\n",
    "\n",
    "precision_top = metrics.precision(test_answer, obtained_words_top)\n",
    "recall_top = metrics.recall(test_answer, obtained_words_top)\n",
    "f1_top = metrics.f1(precision_top, recall_top)\n",
    "\n",
    "print(\"###    Obtained Scores    ###\")\n",
    "print(\"### (full dataset, top 15)###\")\n",
    "print(\"###\")\n",
    "print(\"### Precision : %.4f\" % precision_top)\n",
    "print(\"### Recall    : %.4f\" % recall_top)\n",
    "print(\"### F1        : %.4f\" % f1_top)\n",
    "print(\"###                       ###\")\n",
    "\n",
    "\n",
    "print(\"###                       ###\")\n",
    "print(\"###                       ###\")\n",
    "print(\"###       STEMMING        ###\")\n",
    "print(\"###                       ###\")\n",
    "print(\"###                       ###\")\n",
    "\n",
    "STEM_MODE = metrics.stemMode.both\n",
    "\n",
    "precision = metrics.precision(test_answer, obtained_words,STEM_MODE)\n",
    "recall = metrics.recall(test_answer, obtained_words,STEM_MODE)\n",
    "f1 = metrics.f1(precision, recall)\n",
    "\n",
    "print(\"###    Obtained Scores    ###\")\n",
    "print(\"###     (full dataset)    ###\")\n",
    "print(\"###\")\n",
    "print(\"### Precision : %.4f\" % precision)\n",
    "print(\"### Recall    : %.4f\" % recall)\n",
    "print(\"### F1        : %.4f\" % f1)\n",
    "print(\"###                       ###\")\n",
    "\n",
    "keras_precision = keras_metrics.keras_precision(test_y, output)\n",
    "keras_recall = keras_metrics.keras_recall(test_y, output)\n",
    "keras_f1 = keras_metrics.keras_f1(test_y, output)\n",
    "\n",
    "print(\"###    Obtained Scores    ###\")\n",
    "print(\"###    (fixed dataset)    ###\")\n",
    "print(\"###\")\n",
    "print(\"### Precision : %.4f\" % keras_precision)\n",
    "print(\"### Recall    : %.4f\" % keras_recall)\n",
    "print(\"### F1        : %.4f\" % keras_f1)\n",
    "print(\"###                       ###\")\n",
    "\n",
    "clean_words = postprocessing.get_valid_patterns(obtained_words)\n",
    "\n",
    "precision = metrics.precision(test_answer, clean_words,STEM_MODE)\n",
    "recall = metrics.recall(test_answer, clean_words,STEM_MODE)\n",
    "f1 = metrics.f1(precision, recall)\n",
    "\n",
    "print(\"###    Obtained Scores    ###\")\n",
    "print(\"### (full dataset,        ###\")\n",
    "print(\"###  pos patterns filter) ###\")\n",
    "print(\"###\")\n",
    "print(\"### Precision : %.4f\" % precision)\n",
    "print(\"### Recall    : %.4f\" % recall)\n",
    "print(\"### F1        : %.4f\" % f1)\n",
    "print(\"###                       ###\")\n",
    "\n",
    "obtained_words_top = postprocessing.get_top_words(test_doc, output, 5)\n",
    "\n",
    "precision_top = metrics.precision(test_answer, obtained_words_top,STEM_MODE)\n",
    "recall_top = metrics.recall(test_answer, obtained_words_top,STEM_MODE)\n",
    "f1_top = metrics.f1(precision_top, recall_top)\n",
    "\n",
    "print(\"###    Obtained Scores    ###\")\n",
    "print(\"### (full dataset, top 5) ###\")\n",
    "print(\"###\")\n",
    "print(\"### Precision : %.4f\" % precision_top)\n",
    "print(\"### Recall    : %.4f\" % recall_top)\n",
    "print(\"### F1        : %.4f\" % f1_top)\n",
    "print(\"###                       ###\")\n",
    "\n",
    "obtained_words_top = postprocessing.get_top_words(test_doc, output, 10)\n",
    "\n",
    "precision_top = metrics.precision(test_answer, obtained_words_top,STEM_MODE)\n",
    "recall_top = metrics.recall(test_answer, obtained_words_top,STEM_MODE)\n",
    "f1_top = metrics.f1(precision_top, recall_top)\n",
    "\n",
    "print(\"###    Obtained Scores    ###\")\n",
    "print(\"### (full dataset, top 10)###\")\n",
    "print(\"###\")\n",
    "print(\"### Precision : %.4f\" % precision_top)\n",
    "print(\"### Recall    : %.4f\" % recall_top)\n",
    "print(\"### F1        : %.4f\" % f1_top)\n",
    "print(\"###                       ###\")\n",
    "\n",
    "obtained_words_top = postprocessing.get_top_words(test_doc, output, 15)\n",
    "\n",
    "precision_top = metrics.precision(test_answer, obtained_words_top,STEM_MODE)\n",
    "recall_top = metrics.recall(test_answer, obtained_words_top,STEM_MODE)\n",
    "f1_top = metrics.f1(precision_top, recall_top)\n",
    "\n",
    "print(\"###    Obtained Scores    ###\")\n",
    "print(\"### (full dataset, top 15)###\")\n",
    "print(\"###\")\n",
    "print(\"### Precision : %.4f\" % precision_top)\n",
    "print(\"### Recall    : %.4f\" % recall_top)\n",
    "print(\"### F1        : %.4f\" % f1_top)\n",
    "print(\"###                       ###\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ad7ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
